{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUgbolxu5quo"
      },
      "source": [
        "# Assignment 4: Keyphrase Extraction, Named Entity Recognition & Neural Models\n",
        "\n",
        "Due: Monday, February 06, 2023, at 2pm via Moodle\n",
        "\n",
        "**Team Members** `<Fill out>`\n",
        "\n",
        "Please note that this assignment comes with quite a number of artifacts, totaling somewhere around 5 GB of necessary disk space. In case you are running into issues or do want to keep your environment \"clean\", we suggest the use of [Google Colab](https://colab.research.google.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lB_nf5iQfJRh",
        "outputId": "4109f351-b362-4884-91ec-11072f116077"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.8/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from keybert) (1.21.6)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.8/dist-packages (from keybert) (2.2.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.8/dist-packages (from keybert) (13.2.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (4.4.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (2.6.1)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from rich>=10.4.0->keybert) (2.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2->keybert) (1.2.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (1.13.1+cu116)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.1.97)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.64.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (4.26.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.14.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (3.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (21.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.8/dist-packages (from markdown-it-py<3.0.0,>=2.1.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.3.8->keybert) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers>=0.3.8->keybert) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.3.8->keybert) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-eh5cpb57\n",
            "  Resolved https://github.com/LIAAD/yake to commit 8d71d94ded93fb77f1361f62e5264f19b9c91cd7\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (0.8.10)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (1.21.6)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (3.0)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.8/dist-packages (from yake==0.4.8) (0.9.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from segtok->yake==0.4.8) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.11)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.6)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/LIAAD/yake /tmp/pip-req-build-eh5cpb57\n",
            "ERROR: Could not find a version that satisfies the requirement urllib2 (from versions: none)\n",
            "ERROR: No matching distribution found for urllib2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-202c1901c41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'. ~/.bashrc\\npython3 -m pip install keybert\\npython3 -m pip install git+https://github.com/LIAAD/yake\\npython3 -m pip install transformers\\npython3 -m pip install datasets\\npython3 -m pip install nltk\\npython3 -m pip install spacy\\npython3 -m pip install urllib2\\n# Install necessary packages for all questions\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'. ~/.bashrc\\npython3 -m pip install keybert\\npython3 -m pip install git+https://github.com/LIAAD/yake\\npython3 -m pip install transformers\\npython3 -m pip install datasets\\npython3 -m pip install nltk\\npython3 -m pip install spacy\\npython3 -m pip install urllib2\\n# Install necessary packages for all questions\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "python3 -m pip install keybert\n",
        "python3 -m pip install git+https://github.com/LIAAD/yake\n",
        "python3 -m pip install transformers\n",
        "python3 -m pip install datasets\n",
        "python3 -m pip install nltk\n",
        "python3 -m pip install spacy\n",
        "# Install necessary packages for all questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "315kqIiW2Zhr"
      },
      "source": [
        "## Task 1: Keyphrase Extraction (5 + 3 + 3 + 5) = 16 Points\n",
        "\n",
        "In this task, we will implement our own unsupervised keyphrase extraction (KPE) module utilizing a simple grammatical ruling system, which we apply to a Sherlock Holmes novel.\n",
        "To generate TF-IDF-weighted phrases, we will be using the entire collection of Sir Arthur Donan Coyle novels to calculate document frequencies.\n",
        "\n",
        "Finally, we compare the results to general-purpose KPE libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joME4y0pC-bq"
      },
      "source": [
        "### Sub Task 1: Unsupervised Keyphrase Extraction System (5 Points)\n",
        "\n",
        "#### 1. Candidate Generation\n",
        "We will need to generate a set of suitable candidate phrases first, which can then be ranked as keyphrases later on. To do this, we will again be using spaCy's, this time its rule-based [`Matcher` class](https://spacy.io/api/matcher).\n",
        "\n",
        "The syntactic pattern of a keyphrase candidate should satisfy the following rules:\n",
        "\n",
        "1. An optional adjective, noun, proper noun\n",
        "2. An optional adjective, noun, proper noun\n",
        "3. A mandatory noun or proper noun.\n",
        "\n",
        "Add a second pattern, which recognizes the pattern\n",
        "\n",
        "1. A noun or proper noun\n",
        "2. An adposition\n",
        "3. Another noun or proper noun\n",
        "\n",
        "Note that the first condition will match any phrase of length between 1-3 tokens, which is a suitable approximation for our task at hand, whereas the second pattern is slightly more specific, always matching exactly three tokens.\n",
        "An example of a valid matched phrases for the first pattern would be \"Sherlock Holmes\" ([PROPN, PROPN]), and \"Hounds of Baskervilles\" ([NOUN, ADP, PROPN]) for the second pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "j7yKhziiDkzJ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "from urllib.request import urlopen\n",
        "from collections import Counter\n",
        "from yake import KeywordExtractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "1cFJOPt3DlZu"
      },
      "outputs": [],
      "source": [
        "# load language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define the above patterns\n",
        "patterns = [\n",
        "            [{\"POS\": {\"IN\": [\"ADJ\", \"NOUN\",\"PROPN\"]}, \"OP\": \"*\"},\n",
        "             {\"POS\": {\"IN\": [\"ADJ\", \"NOUN\",\"PROPN\"]}, \"OP\": \"*\"},\n",
        "             {\"POS\": {\"IN\": [\"NOUN\",\"PROPN\"]}}],\n",
        "            \n",
        "            [{\"POS\": {\"IN\": [\"NOUN\",\"PROPN\"]}},\n",
        "             {\"POS\": \"ADP\"},\n",
        "             {\"POS\": {\"IN\": [\"NOUN\",\"PROPN\"]}}]\n",
        "            ]\n",
        "\n",
        "\n",
        "matcher.add(\"Sherlock\", patterns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFawE55KfJRn"
      },
      "source": [
        "To verify whether your pattern is correct, use the below example.\n",
        "If you have done everything correctly, your matcher will identify **13 phrases**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71JZGbIlHfxx",
        "outputId": "7a5ec0b6-d21c-45d2-c5b8-77de7caba1d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(\"This is a simple test. It should return 'simple', and 'test', among other phrases. Maybe we can also see if it can recognize the art of war. Would it recognize integer linear programming, too?\")\n",
        "matches = matcher(doc)\n",
        "\n",
        "print(len(matches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x2TRsI-GeLq"
      },
      "source": [
        "#### 2. Applying Your System\n",
        "\n",
        "Once you have matched the correct number of keyphrase candidates on the above example, apply your rule-based matcher to an actual data sample. We are going to use the Sherlock Holmes novel \"Hounds of Baskervilles\". You can find the raw text file at the following URL:\n",
        "\n",
        "https://sherlock-holm.es/stories/plain-text/houn.txt\n",
        "\n",
        "Download the text from this URL and apply your spaCy model and matcher on it.  \n",
        "**Hint:** Make sure you properly decode your input, since some libraries return binary strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cbDFj7DSLQ5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16b5c37-3ae2-470b-b734-e3b42863cb0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15938\n"
          ]
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "def load_txt_from_url(url: str = \"https://sherlock-holm.es/stories/plain-text/houn.txt\") -> str:\n",
        "  text = \"\"\n",
        "  for line in urlopen(url):\n",
        "    line = line.decode('UTF-8')\n",
        "    text += str(line)\n",
        "  return text\n",
        "\n",
        "text = load_txt_from_url()\n",
        "\n",
        "# Apply the spacy model to the loaded text and extract the phrases with the Matcher\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "print(len(matches))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4U96ELPKtIA"
      },
      "source": [
        "We will now investigate which phrase candidates are the most frequently appearing in this novel, simply based on the phrase frequency. Therefore, convert your abstract match objects into actual strings, lowercase them, and return the 20 most frequently occurring phrase candidates and their respective frequencies.  \n",
        "**Hint:** For counting occurrences, you may look at `collections.Counter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tGof05SIXES",
        "outputId": "be13fad8-1334-4fe1-d743-ba659f31493c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "[('sir', 350), ('man', 214), ('holmes', 192), ('moor', 159), ('henry', 156), ('sir henry', 135), ('watson', 117), ('baskerville', 116), ('dr.', 109), ('charles', 94), ('stapleton', 93), ('mortimer', 89), ('night', 88), ('time', 86), ('sir charles', 86), ('house', 75), ('face', 75), ('hound', 72), ('barrymore', 72), ('eyes', 71)]\n"
          ]
        }
      ],
      "source": [
        "candidates = []\n",
        "# Lowercase and add the extracted candidate matches to `candidates`\n",
        "for match_id,start,end in matches:\n",
        "    string = str(doc[start:end])\n",
        "    string = string.lower()\n",
        "    candidates.append(string)\n",
        "\n",
        "# Count the number of occurrences of different candidate phrases\n",
        "print(type(candidates))\n",
        "candidate_phrases = Counter(candidates)\n",
        "# Print the most frequently occurring phrases, together with the respective frequencies\n",
        "print(candidate_phrases.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYQt8278NA7v"
      },
      "source": [
        "#### 3. Briefly summarize the quality of your top 20 candidates:\n",
        "\n",
        "The Top 20 Candidates are not really sentences, but mostly single words, as the pattern from previously allows amtches of single tokens, that are much more common, than token combinations. This is not really what we wanted. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aucrN3TPJqw1"
      },
      "source": [
        "### Sub Task 2: Generating Document Frequency Values (3 Points)\n",
        "\n",
        "To compare the previously generated terms with a more refined model, we are going to extract document frequencies from the collection of all Sherlock Holmes works. Since the books are relatively long documents, we are instead going to split based on a simple heuristic in the input document, which should allow a decent approximation by taking into account individual chapters of each novel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp7rUN9N218"
      },
      "source": [
        "1. Start by loading the Sherlock Holmes canon from https://sherlock-holm.es/stories/plain-text/cnus.txt  \n",
        "Afterwards, split the full document into individual chapters. For this, use three consecutive line breaks `\\n\\n\\n` as a splitting condition to approximate the chapters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJYRCDJyP-Xp",
        "outputId": "893af2b4-6538-4eaf-b124-e82f3ddd8088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "353\n"
          ]
        }
      ],
      "source": [
        "df_texts = load_txt_from_url(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\")\n",
        "\n",
        "split_df_texts = df_texts.split(\"\\n\\n\\n\")\n",
        "\n",
        "print(len(split_df_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRcMyiKkRU7d"
      },
      "source": [
        "After splitting, you should have 353 individual \"documents\" to work with.\n",
        "\n",
        "2. Now, create a dictionary containing each phrase encountered in the larger corpus, and its associated document frequency. Again, ensure that phrase strings are lowercased for consistency with the previous transformation.  \n",
        "**Hint:** Since the processing of 353 documents might take a while, incorporate [`tqdm.tqdm`](https://tqdm.github.io/) to visualize progress on the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtXpYkWQQJi2",
        "outputId": "b2c3ced1-fcca-4489-b3b1-7f309cf89160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 353/353 [01:09<00:00,  5.05it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "def return_occurring_phrases(doc_text: str) -> List[str]:\n",
        "  # process text with spaCy and apply the Matcher\n",
        "  doc = nlp(doc_text)\n",
        "  matches = matcher(doc)\n",
        "\n",
        "  # Candidates can be a set, since we only care about the occurrence *once* for IDF values.\n",
        "  # Again, extract the lower-cased text of a matched span.\n",
        "  candidates = set()\n",
        "  for match_id, start, end in matches:\n",
        "    string = str(doc[start:end])\n",
        "    string = string.lower()\n",
        "    candidates.add(string)\n",
        "\n",
        "  return list(candidates)\n",
        "\n",
        "all_document_phrases = []\n",
        "# Iterate through the individual documents and extract phrases for them. Use `tqdm` to visualize progress\n",
        "for element in tqdm(split_df_texts):\n",
        "  candidates = return_occurring_phrases(element)\n",
        "  for candidate in candidates:\n",
        "    all_document_phrases.append(candidate)\n",
        "\n",
        "\n",
        "\n",
        "# Once again, count the frequency of term occurrences across all documents\n",
        "phrases = Counter(all_document_phrases)\n",
        "# Print the most frequently occurring phrases, together with the respective frequencies\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd7VZiOKfJRv"
      },
      "source": [
        "3. Output the 20 most frequently appearing document phrases that your system detected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDTmrTLSY2q",
        "outputId": "9f215f4a-18c0-41c2-a011-c8074434c149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('man', 112), ('holmes', 107), ('eyes', 104), ('time', 104), ('night', 104), ('face', 102), ('hand', 102), ('sherlock', 101), ('sherlock holmes', 101), ('way', 101), ('matter', 100), ('room', 99), ('mr.', 98), ('day', 97), ('hands', 97), ('case', 96), ('house', 96), ('life', 96), ('one', 95), ('door', 95)]\n"
          ]
        }
      ],
      "source": [
        "print(phrases.most_common(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR9FmhTpS_TB"
      },
      "source": [
        "### Sub Task 3: Generating Weighted Keyphrases (3 Points)\n",
        "\n",
        "We can now incorporate the extracted keyphrases to calculate `tf-idf` scores, and return a hopefully improved version of our keyphrases for the original \"Hounds of Baskervilles\" document. \n",
        "\n",
        "1. Iterate over all phrases occurring in the novel \"Hounds of Baskervilles\", and re-score phrases according to the definition of TF-IDF. Use the smoothed definition of idf:\n",
        "\n",
        "$ idf(t, D) = \\log \\frac{|D|}{|\\{d \\in D : t \\in d\\}| + 1} + 1 $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "HDDoPiAATWat"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Dict\n",
        "\n",
        "def tf_idf(tf: int, df_count: int) -> float:\n",
        "  \"\"\"\n",
        "  Computes the TF-IDF scores according to the above-mentioned formula.\n",
        "  Note that you may use a constant for the number of documents (|D|).\n",
        "  \"\"\"\n",
        "  idf = math.log(1/1+df_count) + 1\n",
        "\n",
        "  return tf*idf\n",
        "\n",
        "tf_idf_weighted_candidates = []\n",
        "\n",
        "# Iterate through all candidate phrase/frequency pairs and compute the TF-IDF scores for each phrase\n",
        "# Store the phrase together with its TF-IDF score in `tf_idf_weighted_candidates`\n",
        "\n",
        "for candidate, tf in candidate_phrases.items():\n",
        "  df_count = phrases[candidate]\n",
        "  tfidf = tf_idf(tf,df_count)\n",
        "  helpertuple = (candidate, tfidf)\n",
        "  tf_idf_weighted_candidates.append(helpertuple)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSfTPsukVoEF"
      },
      "source": [
        "2. Now print the top 20 candidate phrases by TF-IDF weight, and compare the results to your previous output. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ulSamTUsGP",
        "outputId": "dae3a795-d821-4883-ee6b-b4784b8de275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('sir', 1917.0678850673723), ('man', 1225.660993204441), ('holmes', 1090.9691956078502), ('watson', 640.8484072939502), ('henry', 638.2026227238973), ('moor', 627.165797687464), ('dr.', 553.4515813857234), ('sir henry', 509.29947750237045), ('night', 497.54851081386204), ('time', 486.240590113547), ('baskerville', 451.28312391595506), ('face', 422.6046741172227), ('house', 418.10332338775373), ('eyes', 401.43118486118414), ('charles', 384.5579906156817), ('way', 359.9982600501933), ('hall', 358.8994069531564), ('mr.', 352.49255055847914), ('day', 351.85295115624604), ('case', 351.2067916457131)]\n"
          ]
        }
      ],
      "source": [
        "print(sorted(tf_idf_weighted_candidates,key=lambda a: a[1], reverse = True)[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdmGgOThVmyz"
      },
      "source": [
        "3. Write your insights on the comparison of the results below. Try to theorize why some of the phrases still appear, or why other phrases are no longer present:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Woz-M2c6fJRy"
      },
      "source": [
        "The lists are very similar in the way, that most values from the first list are reappearing. A notable example of an element that is now missing, is \"Mortimer\". A new element, that previously was not present, is \"life\" ore \"one\". A reason for this could be, that we are using the term frequency from various different novels. Maybe the character named \"Mortimer\" does only play a significant role in the Hound of Baskerville, that we used for the first example, therefore he has no high document frequency which is why he might have dropped out of the list. Words like \"life\" or \"one\" are more general and tend to be present across multiple documents whicht could explain why we find them here. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y8eaRPNWHut"
      },
      "source": [
        "4. Give two examples of how you could further improve the list of keyphrase values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yfMZ0etWNj_"
      },
      "source": [
        "1. We could increase the body of work, that the keyphreases are based on, e.g. using more novels of different authors to gain a better understanding of the matter at hand.\n",
        "2. We could filter out stop words to eliminate single tokens like \"sir\" if they are not bound to a second element. Currently we have \"sir\" as well as \"sir henry\" which is not optimal\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeLJbAYfJRz"
      },
      "source": [
        "### Sub Task 4: Apply off-the-shelf Keyphrase Extraction Tools (5 Points)\n",
        "\n",
        "To put the findings of your system into context, compare them with two popular open-source libraries, namely [YAKE!](https://github.com/LIAAD/yake) and [KeyBERT](https://github.com/MaartenGr/KeyBERT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu1PXIoNfJRz"
      },
      "source": [
        "1. First, start by running the document with YAKE!; you may use the default parameters. Print the resulting keyphrases, which by default returns 20 phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP9vOjLufJRz",
        "outputId": "e0fb1f23-81ae-43a3-ddde-57870051f588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sir Henry Baskerville', 9.703823247628816e-06), ('Sir Henry', 1.648100191408312e-05), ('Sir Charles Baskerville', 2.0351834688440424e-05), ('Sir Charles', 3.724150731612377e-05), ('Sir', 0.00010788043646785337), ('Sir Charles death', 0.0001335320154208488), ('Henry Baskerville', 0.00020407970787866275), ('Holmes', 0.00027267641332312374), ('Hall Sir Henry', 0.0002827705937659273), ('Baskerville Hall', 0.00030297998752354974), ('Sherlock Holmes', 0.00030366832269808347), ('friend Sir Henry', 0.0003232815192427155), ('Baskerville Hall Sir', 0.00033497855089080193), ('Henry', 0.0003734244162998363), ('Charles Baskerville', 0.00045232284621121374), ('BASKERVILLES Arthur Conan', 0.00046235145087207454), ('asked Sir Henry', 0.0005001869412629496), ('Sir Henry put', 0.0005065910358925609), ('Arthur Conan Doyle', 0.0005408670117356265), ('Baskerville', 0.0005977236792600148)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "extractor = KeywordExtractor()\n",
        "keywords = extractor.extract_keywords(text)\n",
        "\n",
        "# Print the top 20 keywords\n",
        "print(keywords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcbbVBVrfJRz"
      },
      "source": [
        "2. Compare both runtime efficiency and the extracted phrases with your own system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnD7ZGHwfJRz"
      },
      "source": [
        "The of the shelf solution runs in 6 seconds while our system needs about a Minute of time. Therefore the off the shelf solutions seems to run much more efficiently. The extracted solutions are also totally different from our system. The off the shelf solution extracts only named entities, ignoring the words like night or face, that our solution held, completely. Also the off the shelf solution shares the same problems as ours when it comes to doubles. It also extracts \"Sir Henry Baskerville\" as well as \"Sir Henry\" which is not optimal, as the words stem from the same string. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81emGM-MfJR0"
      },
      "source": [
        "3. Now use the KeyBERT library to extract keyphrases. Importantly, you will need to split the document into separate paragraphs, as the underlying neural model will be unable to handle the complete document as input.  \n",
        "Use the pattern of `\\n\\n` to separate the text into smaller paragraphs, and filter out any empty lines after. An \"empty line\" also constitutes all inputs that only contain newline (`\\n`) or whitespace ` ` characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEuOxbRrfJR0",
        "outputId": "a210afcb-7449-4a96-ec6c-28ccbca32ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1552\n",
            "1527\n"
          ]
        }
      ],
      "source": [
        "# Split the input text according to the specified criteria and filter empty lines out.\n",
        "text = load_txt_from_url()\n",
        "split_text = text.split(\"\\n\\n\")\n",
        "print(len(split_text))\n",
        "\n",
        "bad_text = (\"\",\" \",\"\\n\")\n",
        "\n",
        "for element in split_text:\n",
        "  if element in bad_text:\n",
        "    split_text.remove(element)\n",
        "\n",
        "print(len(split_text))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXohP46SfJR1"
      },
      "source": [
        "4. To ensure consistency between the tools when extracting keyphrases, set the *n*-gram range to `(1,3)`.\n",
        "Otherwise, leave all parameters at the default value, and extract the keyphrases from each paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "10brqzopfJR1"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "\n",
        "# This might take a while to install\n",
        "model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Extract the keyphrases from each split, using the adjusted keyphrase ngram range\n",
        "# Hint: You may pass a list to the extraction function and KeyBERT will automatically handle iteration.\n",
        "extracted_phrases = model.extract_keywords(split_text, keyphrase_ngram_range=(1, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbFshBEjfJR1"
      },
      "source": [
        "5. Combine the predictions of all individual splits into a single list. For this, sum up the prediction scores across all splits.  \n",
        "**Hint:** `collections.defaultdict` makes aggregations like this much easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "EhmhSlBEfJR2"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "def merge_predictions(list_of_predictions: List[List[Tuple]]) -> List[Tuple]:\n",
        "    \"\"\"\n",
        "    Combines lists of predictions into a single list with added scores.\n",
        "    \"\"\"\n",
        "    listelements = []\n",
        "\n",
        "    for liste in list_of_predictions:\n",
        "      for element in liste:\n",
        "          listelements.append(element)\n",
        "\n",
        "    # Initialisation of defaultdict\n",
        "    phrase_dict = defaultdict(int)\n",
        "    \n",
        "    for k, v in listelements:\n",
        "        phrase_dict[k] += v\n",
        "\n",
        "\n",
        "    # Extract the 20 keyphrases with the highest weithgts from `phrase_dict`\n",
        "    #phrase_list = sorted(phrase_dict,key=lambda a: a[1], reverse = False)[:20]\n",
        "    phrase_list = Counter(phrase_dict).most_common(20)\n",
        "\n",
        "    return phrase_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11A5SLFfJR3",
        "outputId": "b448651c-1263-4da8-b47e-e8c97a15bed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('sir henry', 30.826300000000003), ('holmes', 30.425000000000004), ('dr mortimer', 24.266699999999997), ('mortimer', 22.4), ('watson', 21.2683), ('sir charles', 17.5742), ('said holmes', 16.1206), ('sherlock holmes', 15.2558), ('barrymore', 13.793199999999999), ('dr watson', 12.346900000000002), ('mr holmes', 10.4943), ('sir', 10.3469), ('moor', 10.3131), ('mr sherlock holmes', 8.103), ('hound', 7.942399999999999), ('sir henry baskerville', 7.7776), ('henry', 7.257099999999999), ('baskerville hall', 5.9735000000000005), ('sherlock', 5.7341999999999995), ('yes', 5.649700000000001)]\n"
          ]
        }
      ],
      "source": [
        "print(merge_predictions(extracted_phrases))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eocAhefjfJR3"
      },
      "source": [
        "6. Again, evaluate the result and compare it to the other two approaches in terms of extraction quality and extraction speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcuc2-qPfJR3"
      },
      "source": [
        "The extraction of the Keyphrases with KeyBERT took about 2 Minutes and 42 Seconds. This makes it the slowest of all the solutions, even compared to our own solution. It results in a very different list of keyphrases than the other solutions. While the results are mostly named entities, this time there are also some different phrases like \"yes\" or \"moor\". While the results seem to be better, than the ones we got with YAKE, KeyBERT has some of the same problems with doubles as the other two approaches. For example, \"watson\" and \"dr. watson\" both show up in the results. It would have been better, if KeyBERT had recognized these two results as stemming from the same family of strings, merging them into one. While it is less efficient than the other approaches, KeyBERT delivers the best results, as ist has less problems with duplicates than YAKE without relying on the text corpus from the other sherlock holmes novels. This results in \"DR Mortimer\" being in the results again which speaks for the quality of the method. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEon-GQfJR4"
      },
      "source": [
        "## 2. Named Entity Recognition (4 + 5 + 5 = 14 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLr6toKYfJR4"
      },
      "source": [
        "Slightly different, but still operating on the sequence level, is the task of Named Entity Recognition (NER).\n",
        "In this task, we will evaluate the NER capabilities of some more open-source libraries.\n",
        "Particularly, we will also evaluate the utility of NER as a stand-in for Keyphrase Extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUOg01fJR5"
      },
      "source": [
        "### Sub Task 1: Using spaCy NER (4 Points)\n",
        "\n",
        "So far, when using spaCy models, we have primarily disabled the NER component, as it requires significant extra compute.\n",
        "In this task, we will explicitly leave the component enabled, to see what results it can produce on the text from the previous question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29x0-Z2-fJR5"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model, but with NER enabled.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7BB8-hNfJR6"
      },
      "source": [
        "1. Re-load the text for the \"Hounds of Baskervilles\" novel, and run it with the spacy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-imsFHHfJR6"
      },
      "outputs": [],
      "source": [
        "# Re-use the function from the previous exercise.\n",
        "text = ## YOUR CODE\n",
        "\n",
        "doc = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj8x7rbzfJR6"
      },
      "source": [
        "2. Similar to the previous exercise, count the number of occurrences, however, this time for the extracted entities instead of phrases. Print the top 20 most frequently occurring entities.  \n",
        "Make sure to lowercase the text again during your aggregation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0RESPLjfJR7"
      },
      "outputs": [],
      "source": [
        "# Count the number of occurrences of particular entities\n",
        "## YOUR CODE\n",
        "\n",
        "# Print the top 20 most frequently occurring entities.\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdIHmhaZfJR7"
      },
      "source": [
        "You might have noticed some unwanted results in the list, such as \"night\". Upon closer inspection, it turns out that the NER module further differentiates between different entity *categories*, such as PERSON (referencing, as expected, a physical person) or ORG (organizations, such as companies, NGOs, etc.), but also TIME (under which \"night\" falls). For reference, you can find the full list of supported NER labels by this particular model [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
        "\n",
        "3. Refine the list of most common entities by printing out the top three occurring entities in the category `PERSON`, `ORG` and `GPE` (physical locations) instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oINk0v_IfJR7"
      },
      "outputs": [],
      "source": [
        "def get_top_entities_by_class(doc: spacy.tokens.Doc, class_name: str, n: int = 3) -> List[Tuple[str, int]]:\n",
        "    \"\"\"\n",
        "    Returns the three most frequent entities (and their frequencies)\n",
        "    of entity type `class_name` from `doc`.\n",
        "    \"\"\"\n",
        "    # Extract phrase and frequency of a particular entity class\n",
        "    counter = ## YOUR CODE\n",
        "    # Return the top 3 entities and frequencies\n",
        "    return ## YOUR CODE\n",
        "\n",
        "# Print the results for \"PERSON\", \"ORG\" and \"GPE\"\n",
        "print( ## YOUR CODE\n",
        "print( ## YOUR CODE\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUq2oHOJfJR7"
      },
      "source": [
        "### Sub Task 2: Financial Bank Statements of Deutsche Bank (5 Points)\n",
        "\n",
        "Instead of using the Sherlock Holmes Novels, we will now compare the functionality of spaCy and NLTK's NER modules on the financial statements of Deutsche Bank from 2021. For this, see the file available on Moodle.\n",
        "\n",
        "1. Download it and convert the PDF document into text, by using the `pdftotext` command-line utility. In particular, run with the `-layout` option enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ig3Ryf2nfJR8"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        ". ~/.bashrc\n",
        "## YOUR SHELL COMMAND HERE\n",
        "# If you have to execute this command through your shell, still paste the command you ran in here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMJuHWDLfJR8"
      },
      "source": [
        "2. Given that the document is extremely long, split the inputs into chunks of 500.000 characters and process them separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj07pzc4fJR8"
      },
      "outputs": [],
      "source": [
        "def load_long_text_in_chunks(fp: str, chunk_size: int = 500_000) -> List[str]:\n",
        "    \"\"\"\n",
        "    Loads a text file (located at `fp`) and chunks it into chunks fo at most `chunk_size` characters.\n",
        "    Note that the last chunk might be significantly shorter.\n",
        "    \"\"\"\n",
        "    # Load the text file\n",
        "    text = ## YOUR CODE\n",
        "\n",
        "    # Split the text into segments of at most `chunk_size` characters\n",
        "    chunks = ## YOUR CODE\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n84sSixmfJR8"
      },
      "outputs": [],
      "source": [
        "db_chunks = load_long_text_in_chunks( ## YOUR CODE HERE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqWNdEN6fJR9"
      },
      "source": [
        "3. Print the top 5 occurring `ORG` entities that are not referencing Deutsche Bank itself, both by using spaCy's NER module and the NER function of NLTK.  \n",
        "To exclude \"Deutsche Bank\" entities, filter out all entities that contain both \"deutsche\" and \"bank\" in their name, irrespective of the actual upper-/lowercasing.\n",
        "**Hint:** For more information on how to run NER with NLTK, see [here](https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/#performing-ner-with-nltk-and-spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1mBtSf_fJR9"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "org_entities_spacy = []\n",
        "org_entities_nltk = []\n",
        "\n",
        "def is_deutsche_bank_entity(name: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if the entity name contains \"deutsche\" and \"bank\" in some upper-/lowercased version.\n",
        "    This means both \"Deutsche Bank\" and \"deutsche bank's\" should be recognized.\n",
        "    \"\"\"\n",
        "    ## YOUR CODE\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    # Process the chunk with spaCy\n",
        "    doc = ## YOUR CODE\n",
        "\n",
        "    # And also with NLTK\n",
        "    ## YOUR CODE\n",
        "    \n",
        "\n",
        "    # Add all the extracted \"ORG\" entities to `org_entities`, except those referencing Deutsche Bank\n",
        "    org_entities_spacy.extend( ## YOUR CODE\n",
        "    org_entities_nltk.extend( ## YOUR CODE\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDlpELtFfJR9"
      },
      "outputs": [],
      "source": [
        "# Return the top 5 entities by frequency\n",
        "\n",
        "entity_counts_spacy = ## YOUR CODE\n",
        "entity_counts_nltk = ## YOUR CODE\n",
        "\n",
        "print( ## YOUR CODE\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct4M8-IqfJR_"
      },
      "source": [
        "4. Compare and analyze the different results between the two methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fp78s8fJR_"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlB-FEWPfJR_"
      },
      "source": [
        "### Sub Task 3: Co-Occurrence Counts of Entities (5 Points)\n",
        "\n",
        "As is becoming apparent, the *raw* occurrence counts of entities might not be meaningful on its own, especially if we are interested in less frequently occurring entities.\n",
        "\n",
        "Instead, we will \"investigate\" the entities that are most frequently mentioned in association with \"Deutsche Bank\". For this purpose, we will look at the textual co-occurrences of two named entities. The basic idea is that entities that frequently appear together are likely related.\n",
        "\n",
        "1. For each text chunk, extract all mentions of the entity `('Deutsche Bank', 'ORG')`, as well as all `PERSON` entity mentions in the text using spaCy. Store the respective entity name and the text position. Unlike the previous question, you do *not* need to check for different spelllings of the \"Deutsche Bank\" entity.  \n",
        "**Hint:** Entities are represented as a [`Span`](https://spacy.io/api/span) element in spaCy, which has access to text position.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_UYf0wwfJR_"
      },
      "outputs": [],
      "source": [
        "entity_mentions_with_start_position = []\n",
        "\n",
        "for chunk in db_chunks:\n",
        "    chunk_mentions = []\n",
        "    # Process the doc with spaCy\n",
        "    doc = ## YOUR CODE\n",
        "    \n",
        "    # Extract only entity mentions of \"Deutsche Bank\" (ORG) or any PERSON mention.\n",
        "    # Append each mention, including the text and its starting position, to `chunk_mentions`\n",
        "    \n",
        "    # Append the chunk's entities to the aggregate list\n",
        "    entity_mentions_with_start_position.append(chunk_mentions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Occ2qMSfJSA"
      },
      "source": [
        "2. Within each chunk, for each mention of `Deutsche Bank`, search for `PERSON` entities that have a starting position within 200 characters before/after the starting position of the `Deutsche Bank` mention. Count for each `PERSON` entity how many times it occurs nearby a mention of `Deutsche Bank`.  \n",
        "Aggregate the co-occurrences across all chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0hQE5yafJSA"
      },
      "outputs": [],
      "source": [
        "co_occurrences = []\n",
        "\n",
        "for chunk_mentions in entity_mentions_with_start_position:\n",
        "    # Iterate through the entities. If the entity is a \"Deutsche Bank\" mention, extract nearby\n",
        "    # PERSON references (less than +/- 200 character difference in the starting position)\n",
        "\n",
        "    ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlM1keOWfJSA"
      },
      "source": [
        "\n",
        "3. Return the number of co-occurrences and the name of the top 5 frequently occurring `PERSON` entities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O64lZJ8ffJSB"
      },
      "outputs": [],
      "source": [
        "co_occurrence_counts = ## YOUR CODE\n",
        "\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D1s5pnJfJSB"
      },
      "source": [
        "4. Look back at the results of your previous task. Are the `PERSON` entities returned by your co-occurrence method the same ones that appear most frequently by raw counts?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCKd5wfPfJSB"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GgA3mo-fJSB"
      },
      "source": [
        "## 3. Neural Models with Huggingface (3 + 5 + 2 = 10 Points)\n",
        "\n",
        "For state-of-the-art performance, most text-related tasks nowadays use some variation of the Transformer architecture. The particular advantage is especiall the readily available weights for models that have been pre-trained on large general-purpose datasets, which reduces the amount of domain-specific labeled training data.\n",
        "\n",
        "In this task, we will explore the [Huggingface](https://hf.co/) ecosystem to see in which way Transformer models can be used.\n",
        "One of the central aspects of the Huggingface platform is the so-called [Model Hub](https://huggingface.co/models), where you can find many different models uploaded by community members for a variety of tasks.\n",
        "\n",
        "Because the neural models are generally very expensive to run, this exercise will be limited to  less data than in previous questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z_5IpPNfJSC"
      },
      "source": [
        "### Sub Task 1: Loading Transformer Models (3 Points)\n",
        "\n",
        "1. Install the `transformers` library and load the model `cardiffnlp/twitter-roberta-base-sentiment-latest` to classify a sequence.\n",
        "2. Report the result of the prediction on the test sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1moyR_DfJSC"
      },
      "outputs": [],
      "source": [
        "from transformers import ## YOUR IMPORTS\n",
        "\n",
        "model = ## YOUR CODE\n",
        "tokenizer = ## YOUR CODE\n",
        "\n",
        "input_text = \"Das ist ein Test.\"\n",
        "\n",
        "prediction = ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v-zrO7CfJSD"
      },
      "source": [
        "### Sub Task 2: Using Pipelines (5 Points)\n",
        "\n",
        "The most succinct way of using a Transformer model is the [`transformers.pipeline`](https://huggingface.co/docs/transformers/pipeline_tutorial). You can check out the linked tutorial for more information on the topic, but essentially, `pipeline` provides a light-weight wrapper around a number of different popular NLP tasks\n",
        "\n",
        "1. Instead of manually defining a pipeline, now load a model through a `\"text-classification\"` pipeline. Look up the neural model that is loaded by default, and post the link to its [model card](https://huggingface.co/docs/hub/model-cards) below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hy7ng3VfJSD"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amhTDmEdfJSD"
      },
      "source": [
        "2. Now, instead, load a pipeline for `\"text-classification\"`, but with a custom model and tokenizer. Use the Model Hub platform to find the most popular model for the German language (by number of downloads) and manually specify the usage of another model (and tokenizer) to the pipeline. Re-run the previous example, and report the prediction result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIR_ZL0nfJSD"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = ## YOUR CODE\n",
        "tokenizer = ## YOUR CODE\n",
        "\n",
        "# Instantiate the pipeline with custom components\n",
        "pipe = ## YOUR CODE\n",
        "\n",
        "# Output the prediction by your pipe on the test sample.\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a2GheTUfJSE"
      },
      "source": [
        "3. Keeping in line with the previous exercises, let us now try and actually predict something with the model. Re-load a pipeline, this time for Named Entity Recognition, using the default model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZpo8RhXfJSE"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkTYcBg_fJSE"
      },
      "source": [
        "4. Run the pipeline with the text from the Deutsche Bank report from Question 2 and output the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbD7DBS-fJSE"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE\n",
        "\n",
        "print( ## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Ie4DNUfJSF"
      },
      "source": [
        "5. Look at the results. Something looks strange here; why is it not working properly? Elaborate your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KiPO3OEfJSF"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p592U3GifJSF"
      },
      "source": [
        "### Sub Task 3: Using Datasets through Huggingface (2 Points)\n",
        "\n",
        "Instead of using the `transformers` library for model training and inference, it is also possible to use other libraries by Huggingface without neural models.\n",
        "In particular, the `datasets` library provides a centralized and streamlined way of accessing a variety of different datasets.\n",
        "\n",
        "1. Using the `datasets` library, load the `imdb` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uvt3LdjAfJSG"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_twriRafJSG"
      },
      "source": [
        "2. Report the mean length of `text` column for the training, validation and test split, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yUvxvOpfJSH"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "c2042d1922a07c71a3ec2c9c9cbb77e812df8ca713873bc0ea973593272381e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}