{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUgbolxu5quo"
   },
   "source": [
    "# Assignment 4: Keyphrase Extraction, Named Entity Recognition & Neural Models\n",
    "\n",
    "Due: Monday, February 06, 2023, at 2pm via Moodle\n",
    "\n",
    "**Team Members** \n",
    "* BjÃ¶rn Bulkens\n",
    "* Klemens Gerber\n",
    "* Daniel Knorr\n",
    "\n",
    "Please note that this assignment comes with quite a number of artifacts, totaling somewhere around 5 GB of necessary disk space. In case you are running into issues or do want to keep your environment \"clean\", we suggest the use of [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "python3 -m pip install keybert\n",
    "python3 -m pip install git+https://github.com/LIAAD/yake\n",
    "python3 -m pip install transformers\n",
    "python3 -m pip install datasets\n",
    "python3 -m pip install nltk\n",
    "python3 -m pip install spacy\n",
    "# Install necessary packages for all questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "315kqIiW2Zhr"
   },
   "source": [
    "## Task 1: Keyphrase Extraction (5 + 3 + 3 + 5) = 16 Points\n",
    "\n",
    "In this task, we will implement our own unsupervised keyphrase extraction (KPE) module utilizing a simple grammatical ruling system, which we apply to a Sherlock Holmes novel.\n",
    "To generate TF-IDF-weighted phrases, we will be using the entire collection of Sir Arthur Donan Coyle novels to calculate document frequencies.\n",
    "\n",
    "Finally, we compare the results to general-purpose KPE libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joME4y0pC-bq"
   },
   "source": [
    "### Sub Task 1: Unsupervised Keyphrase Extraction System (5 Points)\n",
    "\n",
    "#### 1. Candidate Generation\n",
    "We will need to generate a set of suitable candidate phrases first, which can then be ranked as keyphrases later on. To do this, we will again be using spaCy's, this time its rule-based [`Matcher` class](https://spacy.io/api/matcher).\n",
    "\n",
    "The syntactic pattern of a keyphrase candidate should satisfy the following rules:\n",
    "\n",
    "1. An optional adjective, noun, proper noun\n",
    "2. An optional adjective, noun, proper noun\n",
    "3. A mandatory noun or proper noun.\n",
    "\n",
    "Add a second pattern, which recognizes the pattern\n",
    "\n",
    "1. A noun or proper noun\n",
    "2. An adposition\n",
    "3. Another noun or proper noun\n",
    "\n",
    "Note that the first condition will match any phrase of length between 1-3 tokens, which is a suitable approximation for our task at hand, whereas the second pattern is slightly more specific, always matching exactly three tokens.\n",
    "An example of a valid matched phrases for the first pattern would be \"Sherlock Holmes\" ([PROPN, PROPN]), and \"Hounds of Baskervilles\" ([NOUN, ADP, PROPN]) for the second pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7yKhziiDkzJ"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cFJOPt3DlZu",
    "outputId": "a13cb7f2-71ea-49bc-b2cd-b1626cef4789"
   },
   "outputs": [],
   "source": [
    "# load language model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the above patterns\n",
    "pattern = ## YOUR CODE\n",
    "\n",
    "matcher.add( ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify whether your pattern is correct, use the below example.\n",
    "If you have done everything correctly, your matcher will identify **13 phrases**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71JZGbIlHfxx",
    "outputId": "7b650ae4-99bd-4226-ab95-fe0b8cc2b09e"
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a simple test. It should return 'simple', and 'test', among other phrases. Maybe we can also see if it can recognize the art of war. Would it recognize integer linear programming, too?\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(len(matches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x2TRsI-GeLq"
   },
   "source": [
    "#### 2. Applying Your System\n",
    "\n",
    "Once you have matched the correct number of keyphrase candidates on the above example, apply your rule-based matcher to an actual data sample. We are going to use the Sherlock Holmes novel \"Hounds of Baskervilles\". You can find the raw text file at the following URL:\n",
    "\n",
    "https://sherlock-holm.es/stories/plain-text/houn.txt\n",
    "\n",
    "Download the text from this URL and apply your spaCy model and matcher on it.  \n",
    "**Hint:** Make sure you properly decode your input, since some libraries return binary strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cbDFj7DSLQ5v"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "def load_txt_from_url(url: str = \"https://sherlock-holm.es/stories/plain-text/houn.txt\") -> str:\n",
    "  text = \"\"\n",
    "  for line in urlopen(url):\n",
    "    line = line.decode('UTF-8')\n",
    "    text += str(line)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4U96ELPKtIA"
   },
   "source": [
    "We will now investigate which phrase candidates are the most frequently appearing in this novel, simply based on the phrase frequency. Therefore, convert your abstract match objects into actual strings, lowercase them, and return the 20 most frequently occurring phrase candidates and their respective frequencies.  \n",
    "**Hint:** For counting occurrences, you may look at `collections.Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tGof05SIXES",
    "outputId": "a32bdb06-438d-4268-cdc6-07563c344aab"
   },
   "outputs": [],
   "source": [
    "candidates = []\n",
    "# Lowercase and add the extracted candidate matches to `candidates`\n",
    "## YOUR CODE\n",
    "\n",
    "# Count the number of occurrences of different candidate phrases\n",
    "candidate_phrases = ## YOUR CODE\n",
    "# Print the most frequently occurring phrases, together with the respective frequencies\n",
    "print( ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQt8278NA7v"
   },
   "source": [
    "#### 3. Briefly summarize the quality of your top 20 candidates:\n",
    "\n",
    "YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aucrN3TPJqw1"
   },
   "source": [
    "### Sub Task 2: Generating Document Frequency Values (3 Points)\n",
    "\n",
    "To compare the previously generated terms with a more refined model, we are going to extract document frequencies from the collection of all Sherlock Holmes works. Since the books are relatively long documents, we are instead going to split based on a simple heuristic in the input document, which should allow a decent approximation by taking into account individual chapters of each novel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfp7rUN9N218"
   },
   "source": [
    "1. Start by loading the Sherlock Holmes canon from https://sherlock-holm.es/stories/plain-text/cnus.txt  \n",
    "Afterwards, split the full document into individual chapters. For this, use three consecutive line breaks `\\n\\n\\n` as a splitting condition to approximate the chapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJYRCDJyP-Xp",
    "outputId": "e1251116-c0a3-4697-9163-328b10891bf9"
   },
   "outputs": [],
   "source": [
    "df_texts = load_txt_from_url(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\")\n",
    "\n",
    "split_df_texts = ## YOUR CODE\n",
    "\n",
    "print(len(split_df_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcMyiKkRU7d"
   },
   "source": [
    "After splitting, you should have 353 individual \"documents\" to work with.\n",
    "\n",
    "2. Now, create a dictionary containing each phrase encountered in the larger corpus, and its associated document frequency. Again, ensure that phrase strings are lowercased for consistency with the previous transformation.  \n",
    "**Hint:** Since the processing of 353 documents might take a while, incorporate [`tqdm.tqdm`](https://tqdm.github.io/) to visualize progress on the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtXpYkWQQJi2",
    "outputId": "6d2f3743-c8bd-4e76-e676-fc53d1672189"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "def return_occurring_phrases(doc_text: str) -> List[str]:\n",
    "  # process text with spaCy and apply the Matcher\n",
    "  doc = ## YOUR CODE\n",
    "  matches = ## YOUR CODE\n",
    "\n",
    "  # Candidates can be a set, since we only care about the occurrence *once* for IDF values.\n",
    "  # Again, extract the lower-cased text of a matched span.\n",
    "  candidates = set()\n",
    "  for match_id, start, end in matches:\n",
    "    ## YOUR CODE\n",
    "\n",
    "  return list(candidates)\n",
    "\n",
    "all_document_phrases = []\n",
    "# Iterate through the individual documents and extract phrases for them. Use `tqdm` to visualize progress\n",
    "## YOUR CODE\n",
    "\n",
    "# Once again, count the frequency of term occurrences across all documents\n",
    "## YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Output the 20 most frequently appearing document phrases that your system detected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vDTmrTLSY2q",
    "outputId": "6f4b5c46-750f-44f0-b259-f2536034fc17"
   },
   "outputs": [],
   "source": [
    "print( ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR9FmhTpS_TB"
   },
   "source": [
    "### Sub Task 3: Generating Weighted Keyphrases (3 Points)\n",
    "\n",
    "We can now incorporate the extracted keyphrases to calculate `tf-idf` scores, and return a hopefully improved version of our keyphrases for the original \"Hounds of Baskervilles\" document. \n",
    "\n",
    "1. Iterate over all phrases occurring in the novel \"Hounds of Baskervilles\", and re-score phrases according to the definition of TF-IDF. Use the smoothed definition of idf:\n",
    "\n",
    "$ idf(t, D) = \\log \\frac{|D|}{|\\{d \\in D : t \\in d\\}| + 1} + 1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDDoPiAATWat"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Dict\n",
    "\n",
    "def tf_idf(tf: int, df_count: int) -> float:\n",
    "  \"\"\"\n",
    "  Computes the TF-IDF scores according to the above-mentioned formula.\n",
    "  Note that you may use a constant for the number of documents (|D|).\n",
    "  \"\"\"\n",
    "  return ## YOUR CODE\n",
    "\n",
    "tf_idf_weighted_candidates = []\n",
    "\n",
    "# Iterate through all candidate phrase/frequency pairs and compute the TF-IDF scores for each phrase\n",
    "# Store the phrase together with its TF-IDF score in `tf_idf_weighted_candidates`\n",
    "for candidate, tf in candidate_phrases.items():\n",
    "  tf_idf_weighted_candidates.append( ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSfTPsukVoEF"
   },
   "source": [
    "2. Now print the top 20 candidate phrases by TF-IDF weight, and compare the results to your previous output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8ulSamTUsGP",
    "outputId": "c02da910-d190-43f1-b7b7-6016e73da92a"
   },
   "outputs": [],
   "source": [
    "print(## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdmGgOThVmyz"
   },
   "source": [
    "3. Write your insights on the comparison of the results below. Try to theorize why some of the phrases still appear, or why other phrases are no longer present:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y8eaRPNWHut"
   },
   "source": [
    "4. Give two examples of how you could further improve the list of keyphrase values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yfMZ0etWNj_"
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 4: Apply off-the-shelf Keyphrase Extraction Tools (5 Points)\n",
    "\n",
    "To put the findings of your system into context, compare them with two popular open-source libraries, namely [YAKE!](https://github.com/LIAAD/yake) and [KeyBERT](https://github.com/MaartenGr/KeyBERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, start by running the document with YAKE!; you may use the default parameters. Print the resulting keyphrases, which by default returns 20 phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yake import KeywordExtractor\n",
    "\n",
    "extractor = ## YOUR CODE\n",
    "keywords = ## YOUR CODE\n",
    "\n",
    "# Print the top 20 keywords\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare both runtime efficiency and the extracted phrases with your own system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Now use the KeyBERT library to extract keyphrases. Importantly, you will need to split the document into separate paragraphs, as the underlying neural model will be unable to handle the complete document as input.  \n",
    "Use the pattern of `\\n\\n` to separate the text into smaller paragraphs, and filter out any empty lines after. An \"empty line\" also constitutes all inputs that only contain newline (`\\n`) or whitespace ` ` characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input text according to the specified criteria and filter empty lines out.\n",
    "split_text = ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To ensure consistency between the tools when extracting keyphrases, set the *n*-gram range to `(1,3)`.\n",
    "Otherwise, leave all parameters at the default value, and extract the keyphrases from each paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# This might take a while to install\n",
    "model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Extract the keyphrases from each split, using the adjusted keyphrase ngram range\n",
    "# Hint: You may pass a list to the extraction function and KeyBERT will automatically handle iteration.\n",
    "extracted_phrases = ## YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Combine the predictions of all individual splits into a single list. For this, sum up the prediction scores across all splits.  \n",
    "**Hint:** `collections.defaultdict` makes aggregations like this much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_predictions(list_of_predictions: List[List[Tuple]]) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Combines lists of predictions into a single list with added scores.\n",
    "    \"\"\"\n",
    "    phrase_dict = ## YOUR CODE\n",
    "\n",
    "    # Iterate through all the lists of predictions and add the scores to the correct dict entry\n",
    "    ## YOUR CODE\n",
    "\n",
    "    # Extract the 20 keyphrases with the highest weithgts from `phrase_dict`\n",
    "    phrase_list = ## YOUR CODE\n",
    "\n",
    "    return phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_predictions(extracted_phrases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Again, evaluate the result and compare it to the other two approaches in terms of extraction quality and extraction speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Named Entity Recognition (4 + 5 + 5 = 14 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly different, but still operating on the sequence level, is the task of Named Entity Recognition (NER).\n",
    "In this task, we will evaluate the NER capabilities of some more open-source libraries.\n",
    "Particularly, we will also evaluate the utility of NER as a stand-in for Keyphrase Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Using spaCy NER (4 Points)\n",
    "\n",
    "So far, when using spaCy models, we have primarily disabled the NER component, as it requires significant extra compute.\n",
    "In this task, we will explicitly leave the component enabled, to see what results it can produce on the text from the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjoern/miniconda3/envs/dsta/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "# Load the en_core_web_sm model, but with NER enabled.\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Re-load the text for the \"Hounds of Baskervilles\" novel, and run it with the spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use the function from the previous exercise.\n",
    "text = load_txt_from_url()\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Similar to the previous exercise, count the number of occurrences, however, this time for the extracted entities instead of phrases. Print the top 20 most frequently occurring entities.  \n",
    "Make sure to lowercase the text again during your aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PERSON:henry', 119), ('PERSON:holmes', 115), ('PERSON:watson', 108), ('CARDINAL:one', 90), ('PERSON:mortimer', 74), ('PERSON:charles', 74), ('CARDINAL:two', 56), ('GPE:london', 49), ('ORDINAL:first', 41), ('ORG:stapleton', 31), ('PERSON:stapleton', 25), ('PERSON:barrymore', 25), ('PERSON:sherlock holmes', 22), ('PERSON:henry baskerville', 20), ('FAC:baskerville hall', 20), ('CARDINAL:half', 18), ('PERSON:coombe tracey', 17), ('ORG:i.', 15), ('GPE:devonshire', 14), ('PERSON:baskerville', 14)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Count the number of occurrences of particular entities\n",
    "ents = Counter()\n",
    "\n",
    "for ent in doc.ents:\n",
    "    ent_lower = ent.text.lower()\n",
    "    ent_label = ent.label_\n",
    "    ents[f\"{ent_label}:{ent_lower}\"] += 1\n",
    "\n",
    "# Print the top 20 most frequently occurring entities.\n",
    "print(ents.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed some unwanted results in the list, such as \"night\". Upon closer inspection, it turns out that the NER module further differentiates between different entity *categories*, such as PERSON (referencing, as expected, a physical person) or ORG (organizations, such as companies, NGOs, etc.), but also TIME (under which \"night\" falls). For reference, you can find the full list of supported NER labels by this particular model [here](https://spacy.io/models/en#en_core_web_sm-labels).\n",
    "\n",
    "3. Refine the list of most common entities by printing out the top three occurring entities in the category `PERSON`, `ORG` and `GPE` (physical locations) instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: [('henry', 119), ('holmes', 115), ('watson', 108)]\n",
      "ORG: [('stapleton', 31), ('i.', 15), ('times', 10)]\n",
      "GPE: [('london', 49), ('devonshire', 14), ('england', 13)]\n"
     ]
    }
   ],
   "source": [
    "def get_top_entities_by_class(doc: spacy.tokens.Doc, class_name: str, n: int = 3) -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Returns the three most frequent entities (and their frequencies)\n",
    "    of entity type `class_name` from `doc`.\n",
    "    \"\"\"\n",
    "    # Extract phrase and frequency of a particular entity class\n",
    "    counter = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_ == class_name):\n",
    "            ent_lower = ent.text.lower()\n",
    "            ent_label = ent.label_\n",
    "            counter[ent_lower] += 1\n",
    "    # Return the top 3 entities and frequencies\n",
    "    return counter.most_common(n)\n",
    "\n",
    "# Print the results for \"PERSON\", \"ORG\" and \"GPE\"\n",
    "print(\"Person:\", get_top_entities_by_class(doc, \"PERSON\"))\n",
    "print(\"ORG:\", get_top_entities_by_class(doc, \"ORG\"))\n",
    "print(\"GPE:\", get_top_entities_by_class(doc, \"GPE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Financial Bank Statements of Deutsche Bank (5 Points)\n",
    "\n",
    "Instead of using the Sherlock Holmes Novels, we will now compare the functionality of spaCy and NLTK's NER modules on the financial statements of Deutsche Bank from 2021. For this, see the file available on Moodle.\n",
    "\n",
    "1. Download it and convert the PDF document into text, by using the `pdftotext` command-line utility. In particular, run with the `-layout` option enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    ". ~/.bashrc\n",
    "pdftotext -layout DB_annual_report.pdf DB_annual_report.txt\n",
    "# If you have to execute this command through your shell, still paste the command you ran in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Given that the document is extremely long, split the inputs into chunks of 500.000 characters and process them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def load_long_text_in_chunks(fp: str, chunk_size: int = 500_000) -> list[str]:\n",
    "    \"\"\"\n",
    "    Loads a text file (located at `fp`) and chunks it into chunks fo at most `chunk_size` characters.\n",
    "    Note that the last chunk might be significantly shorter.\n",
    "    \"\"\"\n",
    "    file = open(fp, 'r')\n",
    "    text = file.read()\n",
    "    #clean \n",
    "    cleaned = re.sub(r'\\n', ' ', text)\n",
    "    cleaned = re.sub(r'\\x0c', '', cleaned)\n",
    "    cleaned = re.sub(r'\\t\\x07', '', cleaned)\n",
    "    cleaned = re.sub(r'\\xad', '', cleaned)\n",
    "    cleaned = re.sub(r'\\x07', '', cleaned)\n",
    "\n",
    "    # Split the text into segments of at most `chunk_size` characters\n",
    "    chunks = [cleaned[i:i + chunk_size] for i in range(0, len(cleaned), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chunks = load_long_text_in_chunks(\"./DB_annual_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print the top 5 occurring `ORG` entities that are not referencing Deutsche Bank itself, both by using spaCy's NER module and the NER function of NLTK.  \n",
    "To exclude \"Deutsche Bank\" entities, filter out all entities that contain both \"deutsche\" and \"bank\" in their name, irrespective of the actual upper-/lowercasing.\n",
    "**Hint:** For more information on how to run NER with NLTK, see [here](https://nanonets.com/blog/named-entity-recognition-with-nltk-and-spacy/#performing-ner-with-nltk-and-spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bjoern/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/bjoern/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/bjoern/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/bjoern/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "org_entities_spacy = []\n",
    "org_entities_nltk = []\n",
    "\n",
    "def is_deutsche_bank_entity(name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the entity name contains \"deutsche\" and \"bank\" in some upper-/lowercased version.\n",
    "    This means both \"Deutsche Bank\" and \"deutsche bank's\" should be recognized.\n",
    "    \"\"\"\n",
    "    name_lower = name.lower()\n",
    "    if (\"deutsche\" in name_lower or \"bank\" in name_lower):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "for chunk in db_chunks:\n",
    "    # Process the chunk with spaCy\n",
    "    doc = nlp(chunk)\n",
    "    chunk_ents_spacy = []\n",
    "    chunk_ents_nltk = []\n",
    "    for ent in doc.ents:\n",
    "        ent_label = ent.label_\n",
    "        ent_text = ent.text\n",
    "        is_deutsche_bank = is_deutsche_bank_entity(ent_text)\n",
    "        if ent_label == \"ORG\" and is_deutsche_bank is False:\n",
    "            chunk_ents_spacy.append(ent_text)\n",
    "\n",
    "    # And also with NLTK\n",
    "    for sent in nltk.sent_tokenize(chunk):\n",
    "        for chunk_ in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            is_deutsche_bank = is_deutsche_bank_entity(' '.join(c[0] for c in chunk_))\n",
    "            if hasattr(chunk_, 'label') and chunk_.label() == \"ORGANIZATION\" and is_deutsche_bank is False:\n",
    "                chunk_ents_nltk.append(' '.join(c[0] for c in chunk_))\n",
    "    \n",
    "\n",
    "    # Add all the extracted \"ORG\" entities to `org_entities`, except those referencing Deutsche Bank\n",
    "    org_entities_spacy.extend(chunk_ents_spacy)\n",
    "    org_entities_nltk.extend(chunk_ents_nltk)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Group', 943), ('the Management Board', 295), ('the Supervisory Board', 239), ('Financial Institution', 223), ('Management Board', 129)]\n",
      "[('Group', 745), ('Management Board', 438), ('Supervisory Board', 342), ('IFRS', 175), ('Total', 127)]\n"
     ]
    }
   ],
   "source": [
    "# Return the top 5 entities by frequency\n",
    "\n",
    "ents_spacy = Counter()\n",
    "ents_nltk = Counter()\n",
    "for ent in org_entities_spacy:\n",
    "    ents_spacy[ent] += 1\n",
    "\n",
    "for ent in org_entities_nltk:\n",
    "    ents_nltk[ent] += 1\n",
    "\n",
    "entity_counts_spacy = ents_spacy.most_common(5)\n",
    "entity_counts_nltk = ents_nltk.most_common(5)\n",
    "\n",
    "print(entity_counts_spacy)\n",
    "print(entity_counts_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compare and analyze the different results between the two methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that the first solution \"Group\" is in both spacy and nltk the same except the frequency. The next two only differ because of a \"the\" infront of the ORG name on the spacy side. So it seems, that spacy differs between ORGs which have a \"the\" or no \"the\" infront of their names, nltk group these to one ORG together. So we can see, that if we sum the frequency of \"the Management Board\" and \"Management Board\" by spacy, the result is almost the frequency of \"Management Board\" by nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Co-Occurrence Counts of Entities (5 Points)\n",
    "\n",
    "As is becoming apparent, the *raw* occurrence counts of entities might not be meaningful on its own, especially if we are interested in less frequently occurring entities.\n",
    "\n",
    "Instead, we will \"investigate\" the entities that are most frequently mentioned in association with \"Deutsche Bank\". For this purpose, we will look at the textual co-occurrences of two named entities. The basic idea is that entities that frequently appear together are likely related.\n",
    "\n",
    "1. For each text chunk, extract all mentions of the entity `('Deutsche Bank', 'ORG')`, as well as all `PERSON` entity mentions in the text using spaCy. Store the respective entity name and the text position. Unlike the previous question, you do *not* need to check for different spelllings of the \"Deutsche Bank\" entity.  \n",
    "**Hint:** Entities are represented as a [`Span`](https://spacy.io/api/span) element in spaCy, which has access to text position.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "217\n",
      "175\n",
      "314\n",
      "240\n",
      "361\n",
      "489\n",
      "427\n",
      "646\n",
      "806\n",
      "731\n",
      "1054\n"
     ]
    }
   ],
   "source": [
    "db_entity_mentions_with_start_position = []\n",
    "per_entity_mentions_with_start_position = []\n",
    "\n",
    "for chunk in db_chunks:\n",
    "    db_chunk_mentions = []\n",
    "    per_chunk_mentions = []\n",
    "    # Process the doc with spaCy\n",
    "    doc = nlp(chunk)\n",
    "    \n",
    "    # Extract only entity mentions of \"Deutsche Bank\" (ORG) or any PERSON mention.\n",
    "    # Append each mention, including the text and its starting position, to `chunk_mentions`\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" and ent.text == \"Deutsche Bank\":\n",
    "            ent_tuple = (ent.text, ent.start)\n",
    "            db_chunk_mentions.append(ent_tuple)\n",
    "        elif ent.label_ == \"PERSON\":\n",
    "            ent_tuple = (ent.text, ent.start)\n",
    "            per_chunk_mentions.append(ent_tuple)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Append the chunk's entities to the aggregate list\n",
    "    db_entity_mentions_with_start_position.extend(db_chunk_mentions)\n",
    "    per_entity_mentions_with_start_position.extend(per_chunk_mentions)    \n",
    "\n",
    "    print(len(db_entity_mentions_with_start_position))\n",
    "    print(len(per_entity_mentions_with_start_position))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Sort the mentions by their starting position\n",
    "db_entity_mentions_with_start_position.sort(key=itemgetter(1))\n",
    "per_entity_mentions_with_start_position.sort(key=itemgetter(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Karl von Rohr                                                                                                          ',\n",
       "  133),\n",
       " ('Bernd Leukert                                                                                                          ',\n",
       "  152)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_entity_mentions_with_start_position[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Within each chunk, for each mention of `Deutsche Bank`, search for `PERSON` entities that have a starting position within 200 characters before/after the starting position of the `Deutsche Bank` mention. Count for each `PERSON` entity how many times it occurs nearby a mention of `Deutsche Bank`.  \n",
    "Aggregate the co-occurrences across all chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "co_occurrences = defaultdict(int)\n",
    "\n",
    "for mentions in db_entity_mentions_with_start_position:\n",
    "    # Iterate through the entities. If the entity is a \"Deutsche Bank\" mention, extract nearby\n",
    "    mentions_start = mentions[1]\n",
    "    # PERSON references (less than +/- 200 character difference in the starting position)\n",
    "    for per in per_entity_mentions_with_start_position:\n",
    "        if per[1] > mentions_start - 200 and per[1] < mentions_start + 200:\n",
    "            # Append the PERSON entity to `co_occurrences`\n",
    "            co_occurrences[per[0]] += 1\n",
    "        else:\n",
    "            pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Return the number of co-occurrences and the name of the top 5 frequently occurring `PERSON` entities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('COVID-19', 224),\n",
       " ('MREL', 204),\n",
       " ('Paul Achleitner', 119),\n",
       " ('Main', 119),\n",
       " ('Norbert Winkeljohann', 72)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(co_occurrences.items(), key=lambda fu: fu[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Look back at the results of your previous task. Are the `PERSON` entities returned by your co-occurrence method the same ones that appear most frequently by raw counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MREL', 56),\n",
       " ('COVID-19', 49),\n",
       " ('Paul Achleitner', 30),\n",
       " ('Norbert Winkeljohann', 20),\n",
       " ('Dagmar ValcÃ¡rcel', 19)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count all mentions at positiion 0 of the tuples\n",
    "Counter([x[0] for x in per_entity_mentions_with_start_position]).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are mostly the same, but in different order and they did co-occur more often the they occured in the text itself. Meaning the word \"Deutsche Bank\" was placed multiple times on a range of 200 characters of those entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Models with Huggingface (3 + 5 + 2 = 10 Points)\n",
    "\n",
    "For state-of-the-art performance, most text-related tasks nowadays use some variation of the Transformer architecture. The particular advantage is especiall the readily available weights for models that have been pre-trained on large general-purpose datasets, which reduces the amount of domain-specific labeled training data.\n",
    "\n",
    "In this task, we will explore the [Huggingface](https://hf.co/) ecosystem to see in which way Transformer models can be used.\n",
    "One of the central aspects of the Huggingface platform is the so-called [Model Hub](https://huggingface.co/models), where you can find many different models uploaded by community members for a variety of tasks.\n",
    "\n",
    "Because the neural models are generally very expensive to run, this exercise will be limited to  less data than in previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 1: Loading Transformer Models (3 Points)\n",
    "\n",
    "1. Install the `transformers` library and load the model `cardiffnlp/twitter-roberta-base-sentiment-latest` to classify a sequence.\n",
    "2. Report the result of the prediction on the test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   0,  495,  281,   16,   90,  364,  179, 4500,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-0.0747,  1.2079, -1.0439]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[-0.07466091  1.2078773  -1.0439109 ]\n",
      "1) neutral 0.7233\n",
      "2) negative 0.2006\n",
      "3) positive 0.0761\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "input_text = \"Das ist ein Test.\"\n",
    "\n",
    "tok_input_text = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(tok_input_text)\n",
    "\n",
    "output = model(**tok_input_text)\n",
    "print(output)\n",
    "\n",
    "scores = output[0][0].detach().numpy()\n",
    "print(scores)\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x)/np.exp(x).sum())\n",
    "\n",
    "scores = softmax(scores)\n",
    "\n",
    "task='sentiment'\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 2: Using Pipelines (5 Points)\n",
    "\n",
    "The most succinct way of using a Transformer model is the [`transformers.pipeline`](https://huggingface.co/docs/transformers/pipeline_tutorial). You can check out the linked tutorial for more information on the topic, but essentially, `pipeline` provides a light-weight wrapper around a number of different popular NLP tasks\n",
    "\n",
    "1. Instead of manually defining a pipeline, now load a model through a `\"text-classification\"` pipeline. Look up the neural model that is loaded by default, and post the link to its [model card](https://huggingface.co/docs/hub/model-cards) below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8571606874465942}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "classifier(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default model is distilbert-base-uncased-finetuned-sst-2-english [Model Card](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get model card metadata\n",
    "print(classifier.modelcard)\n",
    "# Disappointingly, the model card function is not used here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now, instead, load a pipeline for `\"text-classification\"`, but with a custom model and tokenizer. Use the Model Hub platform to find the most popular model for the German language (by number of downloads) and manually specify the usage of another model (and tokenizer) to the pipeline. Re-run the previous example, and report the prediction result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'de', 'score': 0.9930722117424011}]\n"
     ]
    }
   ],
   "source": [
    "# XLM Roberta base language model detection 970,818 downloads\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"papluca/xlm-roberta-base-language-detection\")\n",
    "\n",
    "# Instantiate the pipeline with custom components\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Output the prediction by your pipe on the test sample.\n",
    "print( pipe(\"Das ist ein Testtext um den Text zu testen.\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Keeping in line with the previous exercises, let us now try and actually predict something with the model. Re-load a pipeline, this time for Named Entity Recognition, using the default model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run the pipeline with the text from the Deutsche Bank report from Question 2 and output the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "hf_entities = []\n",
    "for chunks in db_chunks:\n",
    "\n",
    "    chunk_hf_entites = pipe(chunks)\n",
    "    hf_entities.extend(chunk_hf_entites)\n",
    "\n",
    "print(len(hf_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique entities\n",
    "unique_hf_entities = set([x[\"word\"] for x in hf_entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_hf_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'##A',\n",
       " '##MC',\n",
       " '##MF',\n",
       " '##R',\n",
       " '##TC',\n",
       " '##a',\n",
       " '##ab',\n",
       " '##cker',\n",
       " '##d',\n",
       " '##elli',\n",
       " '##en',\n",
       " '##era',\n",
       " '##hl',\n",
       " '##hn',\n",
       " '##io',\n",
       " '##ke',\n",
       " '##ller',\n",
       " '##lt',\n",
       " '##m',\n",
       " '##mann',\n",
       " '##oh',\n",
       " '##r',\n",
       " '##riz',\n",
       " '##rt',\n",
       " '##t',\n",
       " '##uke',\n",
       " '##ulatory',\n",
       " '##vie',\n",
       " '##wing',\n",
       " '##yl',\n",
       " '##Ã',\n",
       " '##Ã¼',\n",
       " 'A',\n",
       " 'Act',\n",
       " 'Alexander',\n",
       " 'Article',\n",
       " 'As',\n",
       " 'Bank',\n",
       " 'Bern',\n",
       " 'Board',\n",
       " 'CC',\n",
       " 'CF',\n",
       " 'Camp',\n",
       " 'Capital',\n",
       " 'Christian',\n",
       " 'Commission',\n",
       " 'Credit',\n",
       " 'Deutsche',\n",
       " 'Dodd',\n",
       " 'Dr',\n",
       " 'F',\n",
       " 'Forum',\n",
       " 'Frank',\n",
       " 'Garth',\n",
       " 'Group',\n",
       " 'Hammond',\n",
       " 'Hermann',\n",
       " 'I',\n",
       " 'James',\n",
       " 'John',\n",
       " 'Josef',\n",
       " 'Karl',\n",
       " 'Kimberly',\n",
       " 'Ku',\n",
       " 'L',\n",
       " 'Lambert',\n",
       " 'Le',\n",
       " 'Lewis',\n",
       " 'M',\n",
       " 'Management',\n",
       " 'Marcus',\n",
       " 'Math',\n",
       " 'Mo',\n",
       " 'Model',\n",
       " 'More',\n",
       " 'Nicolas',\n",
       " 'O',\n",
       " 'R',\n",
       " 'RC',\n",
       " 'Rebecca',\n",
       " 'Reg',\n",
       " 'Release',\n",
       " 'Riley',\n",
       " 'Risk',\n",
       " 'Ritchie',\n",
       " 'S',\n",
       " 'Sc',\n",
       " 'Se',\n",
       " 'Short',\n",
       " 'Simon',\n",
       " 'St',\n",
       " 'States',\n",
       " 'Stefan',\n",
       " 'Stein',\n",
       " 'Stuart',\n",
       " 'Unit',\n",
       " 'United',\n",
       " 'Werner',\n",
       " 'World',\n",
       " 'and',\n",
       " 'von',\n",
       " 'zur'}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_hf_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Deutsche': 12,\n",
       "         'Bank': 15,\n",
       "         'Reg': 1,\n",
       "         '##ulatory': 1,\n",
       "         'Credit': 1,\n",
       "         'Risk': 3,\n",
       "         'Model': 1,\n",
       "         'Forum': 1,\n",
       "         'RC': 2,\n",
       "         '##R': 2,\n",
       "         '##MF': 1,\n",
       "         '##MC': 1,\n",
       "         'and': 1,\n",
       "         'Capital': 1,\n",
       "         'Management': 7,\n",
       "         'As': 1,\n",
       "         'I': 2,\n",
       "         '##A': 1,\n",
       "         'Article': 1,\n",
       "         'L': 5,\n",
       "         'O': 6,\n",
       "         '##TC': 7,\n",
       "         'CC': 1,\n",
       "         'Dodd': 2,\n",
       "         'Frank': 4,\n",
       "         'Act': 2,\n",
       "         'CF': 1,\n",
       "         'United': 1,\n",
       "         'States': 1,\n",
       "         'Release': 1,\n",
       "         'Unit': 1,\n",
       "         'Commission': 3,\n",
       "         'Group': 11,\n",
       "         'World': 1,\n",
       "         'Board': 4,\n",
       "         'Christian': 2,\n",
       "         'Se': 1,\n",
       "         '##wing': 1,\n",
       "         'Karl': 1,\n",
       "         'von': 3,\n",
       "         'R': 1,\n",
       "         '##oh': 1,\n",
       "         '##r': 1,\n",
       "         'F': 1,\n",
       "         '##ab': 1,\n",
       "         '##riz': 1,\n",
       "         '##io': 1,\n",
       "         'Camp': 1,\n",
       "         '##elli': 1,\n",
       "         'Bern': 1,\n",
       "         '##d': 1,\n",
       "         'Le': 1,\n",
       "         '##uke': 1,\n",
       "         '##rt': 1,\n",
       "         'Stuart': 1,\n",
       "         'Lewis': 1,\n",
       "         'James': 1,\n",
       "         'Mo': 1,\n",
       "         '##lt': 1,\n",
       "         '##ke': 2,\n",
       "         'Alexander': 1,\n",
       "         'zur': 1,\n",
       "         'M': 1,\n",
       "         '##Ã¼': 2,\n",
       "         '##hl': 1,\n",
       "         '##en': 1,\n",
       "         '##a': 1,\n",
       "         'Riley': 1,\n",
       "         'Rebecca': 1,\n",
       "         'Short': 1,\n",
       "         'Stefan': 1,\n",
       "         'Simon': 1,\n",
       "         'Ku': 1,\n",
       "         '##hn': 1,\n",
       "         'Werner': 1,\n",
       "         'Stein': 1,\n",
       "         '##m': 1,\n",
       "         '##ller': 1,\n",
       "         'S': 1,\n",
       "         '##yl': 1,\n",
       "         '##vie': 1,\n",
       "         'Math': 1,\n",
       "         '##era': 1,\n",
       "         '##t': 1,\n",
       "         'Garth': 1,\n",
       "         'Ritchie': 1,\n",
       "         'St': 1,\n",
       "         '##Ã': 1,\n",
       "         'Nicolas': 1,\n",
       "         'More': 1,\n",
       "         'Kimberly': 1,\n",
       "         'Hammond': 1,\n",
       "         'Dr': 1,\n",
       "         'Marcus': 1,\n",
       "         'Sc': 1,\n",
       "         'John': 1,\n",
       "         'Hermann': 1,\n",
       "         'Josef': 2,\n",
       "         'Lambert': 1,\n",
       "         'A': 1,\n",
       "         '##cker': 1,\n",
       "         '##mann': 1})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of unique entities\n",
    "Counter([x[\"word\"] for x in hf_entities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Look at the results. Something looks strange here; why is it not working properly? Elaborate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three obivious problem with the results:\n",
    "\n",
    "1. Tokens which are only one part of word (###) are also returned as entity, which is not really helpful\n",
    "2. Entities which consist of multiple words are split up (deutsche Bank, names, ...)\n",
    "3. There are only a fraction of the entites that were found by spacy\n",
    "--> This is also meant for the occurences of a unique entity like \"Deutsche\", which has only been found 12 times instead of around 730 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db_entity_mentions_with_start_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub Task 3: Using Datasets through Huggingface (2 Points)\n",
    "\n",
    "Instead of using the `transformers` library for model training and inference, it is also possible to use other libraries by Huggingface without neural models.\n",
    "In particular, the `datasets` library provides a centralized and streamlined way of accessing a variety of different datasets.\n",
    "\n",
    "1. Using the `datasets` library, load the `imdb` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|ââââââââââ| 4.31k/4.31k [00:00<00:00, 2.43MB/s]\n",
      "Downloading metadata: 100%|ââââââââââ| 2.17k/2.17k [00:00<00:00, 900kB/s]\n",
      "Downloading readme: 100%|ââââââââââ| 7.59k/7.59k [00:00<00:00, 4.43MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imdb/plain_text to /home/bjoern/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|ââââââââââ| 84.1M/84.1M [00:23<00:00, 3.59MB/s]\n",
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/bjoern/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3/3 [00:00<00:00, 327.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Report the mean length of `text` column for the training, validation and test split, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of train set: 1325.06964\n",
      "Mean length of unsupervised set: 1329.9025\n",
      "Mean length of test set: 1293.7924\n"
     ]
    }
   ],
   "source": [
    "mean_length_train = np.mean([len(x) for x in imdb[\"train\"][\"text\"]])\n",
    "\n",
    "# There is no validation set only one called \"unsupervised\" where the labels are -1 (unkown)\n",
    "mean_length_val = np.mean([len(x) for x in imdb[\"unsupervised\"][\"text\"]])\n",
    "mean_length_test = np.mean([len(x) for x in imdb[\"test\"][\"text\"]])\n",
    "\n",
    "print(f\"Mean length of train set: {mean_length_train}\")\n",
    "print(f\"Mean length of unsupervised set: {mean_length_val}\")\n",
    "print(f\"Mean length of test set: {mean_length_test}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dsta')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c4c7737236c0fb65c52c1fde0665403eaccc71fc442c0a822d5b25eebed90b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
